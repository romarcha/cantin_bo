--------------------------------------------------------------------------
---------              BAYESIAN-OPTIMIZATION                     ---------
--------------------------------------------------------------------------


This is an efficient, C++ implementation of several Bayesian optimization
algorithms. See References for some of the papers.

Basically, it combines the use of an stochastic process as a surrogate function
with the use of some "active learning" criterion. to find the optimum an "arbitrary" 
function using very few iterations.

It can also be used for sequential experimental design and stochastic bandits by
changing the criterion.

------------------------------
INDEX:
-------------------------------
1-Dependencies
2-Install
3-Usage
4-Known Issues
5-References
-------------------------------


***********************
* 1 -  DEPENDENCIES:  *
***********************

1.1 - BOOST:
============

This code uses Boost libraries for matrix operations (uBlas) and random
number generation. They can be found in standard linux distributions or
it can be downloaded from (www.boost.org). Since they are pure template
libraries, they do not require compilation. Just make sure the headers are
on the include path.

They are not very efficient, so it may change in future versions.


1.2 - Nonlinear Optimization library:
=====================================

This library requires some other nonlinear optimization library 
(e.g.: DIRECT). 


a) Using Fortran DIRECT:

For completeness, this code includes a Fortran 77 implementation of the
DIRECT-L algorithm by J. Gablonsky

J. M. Gablonsky and C. T. Kelley, "A locally-biased form of the DIRECT 
algorithm," J. Global Optimization, vol. 21 (1), p. 27-37 (2001). 

The original code can be downloaded from 
http://www4.ncsu.edu/~ctk/SOFTWARE/DIRECTv204.tar.gz
which includes some parallel processing functions that are not yet 
supported in bayesian-optimization.

I have only tested this code using gfortran on Windows, Mac OS X
and Linux, but it should work with other fortran compilers like f77 or
f95.

b) Using NLOPT (default):

We recommend the use of NLOPT for the inner loop optimization. The latest
version can be downloaded from 

http://ab-initio.mit.edu/wiki/index.php/NLopt

NLOPT does not require external libraries and it is compatible with 
Windows and Mac. As compiling it in Windows is tricky, there are 
precompilled dlls for download.

1.3 - PYTHON:
=============

The library has been tested with Python 2.6 and 2.7 Python development files such as 
Python.h are needed to compile the interface. It you want to modify the Python
interface, you also need Cython.

$ cython --cplus bayesopt.pyx


*****************
* 2 - INSTALL:  *
*****************

Bayesian-optimization uses standard C/C++ code and it can be compiled in 
different platforms using CMake.

a) Linux or Mac OS:
-------------------
In Ubuntu/Debian, you can get the dependencies by running:

>> sudo apt-get install libboost-dev python-dev gfortran cmake g++

To compile the source code:

>> cmake . 
>> make
>> sudo make install

Using ccmake instead of cmake you will access a interface to select features 
such as debug mode, which version of DIRECT to use (see below) and if you want
to use shared libraries or not. Shared libraries are required to use the 
Python interface.

If you have doxygen installed on your computer, you can compile the
documentation directly using cmake. 

>> cmake .
>> make
>> make doc
>> sudo make install

The documentation will appear in the "doc" subdirectory.


b) Windows:
-----------
It can be compilled using CMake and MinGW compilers, although
it might get tricky to set the correct paths.



****************
* 3 - USAGE:   *
****************

Jointly with bayesian-optimization, the test program krigtest will be compiled.
It can be used as an example of the interfaces that bayesian-optimization provide.
There are three kind of interfaces.

3.1 - C functional usage
------------------------

This interface is fully functional from C and C++. It resembles the classic 
NLOPT interface, therefore, the NLOPT manual can used as well. We just need to 
define a function pointer to the function that we need to evaluate. The 
function pointer must agree with the template provided in krigwpr.h

Note that the gradient has been included for future compatibility, although
in the current implementation, it is not used. You can just use a NULL pointer.



3.2 - C++ polymorphic usage
---------------------------

The second way to use the function is by creating an object that inherits 
from the SKO object defined in krigging.hpp 

Then, we just need to define the virtual function evaluateSample, which 
has interfaces both for C arrays and uBlas vectors. You can just redefine
your favorite interface.

We may also need to create an object of any of the non parametric processes
to set the desired parameters.



3.3 - Python functional usage
-----------------------------

The file bin/test.py provides an example of the Python interface. It is similar
the C interface. The parameters must be defined as a Python dictionary and the
surrogate function and criterium are selected using the corresponding string.



********************
* 4. KNOWN ISSUES  *
********************

- In some systems, the linker is not able to find the shared libraries. You
just need to point the LD_LIBRARY_PATH and PYTHONPATH to the corresponding
folder (by default: /usr/local/lib). There is a shell script to do that,
exportlocalpaths.sh



********************
* 5. REFERENCES    *
********************

[1]Eric Brochu, Vlad M Cora, and Nando de Freitas. A tutorial on bayesian optimization of expensive
cost functions, with application to active user modeling and hierarchical reinforcement
learning. eprint arXiv:1012.2599, arXiv.org, December 2010.

[2] Mark S. Handcock and Michael L. Stein. A bayesian analysis of kriging. Technometrics,
35(4):403–410, 1993.

[3] Matthew Hoffman, Eric Brochu, and Nando de Freitas. Portfolio allocation for Bayesian 
optimization.
In 27th Conference on Uncertainty in Artificial Intelligence (UAI2011), 2011.

[4] D. Huang, T. T. Allen, W. I. Notz, and N. Zeng. Global optimization of stochastic black-box
systems via sequential kriging meta-models. Journal of Global Optimization, 34(3):441– 466,
2006.

[5] Steven G. Johnson. The NLopt nonlinear-optimization package, http://ab-initio.mit.edu/nlopt.

[6] D.R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal
of Global Optimization, 21:345–383, 2001.

[7] D.R. Jones, M. Schonlau, and W.J. Welch. Efficient global optimization of expensive blackbox
functions. Journal of Global Optimization, 13(4):455–492, 1998.

[8] R. Martinez-Cantin, N. de Freitas, E. Brochu, J.A. Castellanos, and A. Doucet. A Bayesian
exploration-exploitation approach for optimal online sensing and planning with a visually
guided mobile robot. Autonomous Robots - Special Issue on Robot Learning, Part B, 27(3):93–
103, 2009.

[9] Ruben Martinez-Cantin, Nando de Freitas, Jose Castellanos and Arnaud Doucet (2007) Active 
Policy Learning for Robot Planning and Exploration under Uncertainty. In Proc. of Robotics: 
Science and Systems

[10] Jonas Mockus. Application of bayesian approach to numerical methods of global and stochastic
optimization. Journal of Global Optimization, 4:347–365, 1994.

[11] J. Sacks, W.J. Welch, T.J. Mitchell, and H.P. Wynn. Design and analysis of computer experiments.
Statistical Science, 4(4):409–423, 1989.

[12] T.J. Santner, B. Williams, and W. Notz. The Design and Analysis of Computer Experiments.
Springer-Verlag, 2003.

[13] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit
setting: No regret and experimental design. In Proc. International Conference on Machine
Learning (ICML), 2010.

[14] B J Williams, T J Santner, and W I Notz. Sequential design of computer experiments to
minimize integrated response functions. Statistica Sinica, 10(4):1133–1152, 2000.
